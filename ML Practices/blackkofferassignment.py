# -*- coding: utf-8 -*-
"""BlackKofferAssignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i6k62wx6EsUXCU63O4k2WpD25ynRfdau

# Data Extraction an Text Analysis
"""

import pandas as pd
import numpy as np

res = pd.read_excel('/content/cik_list.xlsx')
result = res.copy()
res.info()

res.head()

res.shape

from bs4 import BeautifulSoup as bs
import urllib
from urllib.request import Request, urlopen
pre = 'https://www.sec.gov/Archives/'

"""## Extracting Derived variables"""

!pip install --upgrade pip setuptools wheel
!pip install nltk

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize
import statistics

#import cookielib
contraining_words_whole_report = []

#Creating positive and negative dictionaries
master = pd.read_excel('/content/LoughranMcDonald_MasterDictionary_2018.xlsx')
pos = master['Word'].loc[master.Positive != 0]
neg = master['Word'].loc[master.Negative != 0]

#Number of StopWords
stopwords = open('/content/StopWords_GenericLong.txt','r+')
stopwords = list(bs(stopwords.read()))

#Uncertain and ConstrainDictionaries
uncertain = pd.read_excel('/content/uncertainty_dictionary.xlsx')
constrain = pd.read_excel('/content/constraining_dictionary.xlsx')
uncertain = list(uncertain['Word'])
constrain = list(constrain['Word'])

src = list(res['SECFNAME'])

def eval(df):
  #Get text
  url = pre+df['SECFNAME']
  hdr = {'User-Agent': 'Mozilla/5.0','Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}
  req = Request(url,headers=hdr)
  try:
    global html
    html = urlopen(url).read()
  except urllib.error.HTTPError as e:
    print(e.reason)
  soup = bs(html)
  soup = soup.get_text()

    #Tokenize words and sentences
  word_token = word_tokenize(soup)
  sen_token = sent_tokenize(soup)

    #Cleaning using Stopwords
  cleanedData = [word for word in word_token if word not in stopwords]

    #Extracting derived variables
  pos_count = 0                       #postive score
  for word in pos:
    for w in cleanedData:
      if word == w:
        pos_count = pos_count +1

  neg_count = 0               #negative score
  for word in neg:
    for w in cleanedData:
      if word == w:
        neg_count = neg_count +1
    
    #polarity
  polarity = (pos_count-neg_count)/((pos_count+neg_count)+0.000001)

    #subjectivity
  subjective = (pos_count+neg_count)/(len(word_token)+0.000001)

    ###Analysis of Readability
  length_list = [len(word) for word in word_token]
  avg_sent_len = float(format(statistics.mean(length_list), '.2f'))
    
    #complex_word
  syllable_count = 0 
  complex_word = []
  for word in word_token:
    for w in word:
      if (w=='a' or w=='e' or w=='i' or w=='o' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U'):
        syllable_count=syllable_count+1
      if syllable_count > 2:
        complex_word.append(word)

  complex_word = list(set(complex_word))
  per_complex_word = len(complex_word)/len(word_token)

    #Fog Index
  fog_index = 0.4 * (avg_sent_len + per_complex_word)

    #Average number of words per sentence
  avg_num_words = len(word_token)/len(sen_token)

    #Constraining and Uncertainity Scores
  uncertain_score = len([word for word in word_token if word in uncertain])
  constrain_score = len([word for word in word_token if word in constrain])

    #positive word proportion
  pos_ratio = pos_count/len(word_token)

    #negative word proportion
  neg_ratio = neg_count/len(word_token)

    #constrain word proportion
  constrain_ratio = constrain_score/len(word_token)

    #uncertain word proportion
  uncertain_ratio = uncertain_score/len(word_token)
    
    #Constraining word for the whole report
  contraining_words_whole_report.append(constrain_score)

    #print(source,neg_ratio, constrain_score, polarity, fog_index)
  df['positive_score'] = pos_count
  df['negative_score'] = neg_count
  df['polarity_score'] = polarity
  df['average_sentence_length'] = avg_sent_len
  df['percentage_of_complex_words'] = per_complex_word
  df['fog_index'] = fog_index
  df['complex_word_count'] = len(complex_word)
  df['word_count'] = len(word_token)
  df['uncertainty_score'] = uncertain_score
  df['constraining_score'] = constrain_score
  df['positive_word_proportion'] = pos_ratio
  df['negative_word_proportion'] = neg_ratio
  df['uncertainty_word_proportion'] = uncertain_ratio
  df['constraining_word_proportion'] = constrain_ratio
  
  return df

new = result.apply(eval,axis=1)

#new['contraining_words_whole_report'] = contraining_words_whole_report
new.head()

new.shape

new.to_excel('output.xlsx')

